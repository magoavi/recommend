{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommendation Engine<a class='tocSkip'> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T14:12:12.519930Z",
     "start_time": "2018-05-24T14:12:12.516280Z"
    }
   },
   "source": [
    "# TOC: <a class='tocSkip'>\n",
    "\n",
    "* [1. The Netflix Prize](#The-Netflix-Prize)\n",
    "* [2. Limitations](#Limitations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T12:02:08.555941Z",
     "start_time": "2018-06-07T12:02:08.549919Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading Libraries\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "from scipy.sparse.linalg import svds;\n",
    "import csv;\n",
    "from collections import defaultdict;\n",
    "from collections import Counter;\n",
    "from pandas import DataFrame;\n",
    "from sklearn.model_selection import ParameterGrid;\n",
    "from sklearn.model_selection import KFold as skKFold;\n",
    "import pickle;\n",
    "import timeit;\n",
    "import operator;\n",
    "import math;\n",
    "from __future__ import (absolute_import, division, print_function, unicode_literals);\n",
    "import time;\n",
    "import datetime;\n",
    "import random;\n",
    "import six;\n",
    "from tabulate import tabulate;\n",
    "\n",
    "import statsmodels.api as sm;\n",
    "from sklearn.metrics import mean_squared_error;\n",
    "from sklearn import linear_model;\n",
    "from sklearn import metrics;\n",
    "import statsmodels;\n",
    "from sklearn.neural_network import MLPClassifier;\n",
    "from sklearn.neural_network import MLPRegressor;\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.preprocessing import LabelEncoder;\n",
    "\n",
    "from surprise import Dataset;\n",
    "from surprise.model_selection import cross_validate;\n",
    "from surprise.model_selection import KFold;\n",
    "from surprise import NormalPredictor;\n",
    "from surprise import BaselineOnly;\n",
    "from surprise import KNNBasic;\n",
    "from surprise import KNNWithMeans;\n",
    "from surprise import KNNBaseline;\n",
    "from surprise import SVD;\n",
    "from surprise import SVDpp;\n",
    "from surprise import NMF;\n",
    "from surprise import SlopeOne;\n",
    "from surprise import Reader;\n",
    "from surprise.model_selection import train_test_split;\n",
    "from surprise import CoClustering;\n",
    "from surprise import dump;\n",
    "from surprise import accuracy;\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The Netflix Prize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Methodology\n",
    "\n",
    "The **surprise** package was used for this exercise since that allowed for much faster calculations. The methodology for this project was inspired from [(Jahrer, Töscher and Legenstein, 2010)](#Jahrer,-M.,-Töscher,-A.-and-Legenstein,-R.-2010.-Combining-Predictions-for-Accurate-Recommender-Systems.-[ebook]-commendo.-Available-at:---). The picture below briefly explains the methodology that was adopted for this part of the question. Some essential details missing from the picture below include:\n",
    "\n",
    "* The eight algorithms were trained on the whole training set for the final predictions which would be blended.\n",
    "\n",
    "* The blender was trained on the full probe set after recording the rmse. This was only done after the model was selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Code Plan](viz/code_plan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T12:25:43.197124Z",
     "start_time": "2018-06-07T12:25:43.193799Z"
    }
   },
   "source": [
    "> **Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following code (can be accessed by clicking on the `show code` button at the top of this document) shows how the eigth algorithms were used to create the probe set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T12:41:51.510461Z",
     "start_time": "2018-06-07T12:41:51.498301Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # preparing data\n",
    "\n",
    "# # Loading big data\n",
    "# netflix_train_datapath = r'netflix/Netflix_training.txt'\n",
    "# netflix_train_df = pd.read_table(netflix_train_datapath, delimiter=\",\",\n",
    "#                                names=['movie_id', 'user_id', 'ratings'],\n",
    "#                                dtype={'movie_id':np.uint32, 'user_id':np.uint32, \n",
    "#                                       'ratings':np.float})\n",
    "    \n",
    "# # Test Set\n",
    "# netflix_test_datapath = r'netflix/Netflix_test.txt'\n",
    "# netflix_test_df = pd.read_table(netflix_test_datapath, delimiter=\",\",\n",
    "#                                names=['movie_id', 'user_id', 'ratings'],\n",
    "#                                dtype={'movie_id':np.uint32, 'user_id':np.uint32, \n",
    "#                                       'ratings':np.float})\n",
    "\n",
    "# reader = Reader(rating_scale=(1, 5))\n",
    "# # Re-ordering the columns\n",
    "# cols = netflix_train_df.columns.tolist()\n",
    "# reordered_cols = ['user_id', 'movie_id', 'ratings']\n",
    "# netflix_train_df = netflix_train_df[reordered_cols]\n",
    "# # Rename the columns\n",
    "# netflix_train_df.columns = ['userID', 'itemID', 'rating']\n",
    "# data = Dataset.load_from_df(netflix_train_df, reader)\n",
    "# # Splitting\n",
    "# np.random.seed(0)\n",
    "# random.seed(0)\n",
    "# trainset, testset = train_test_split(data, test_size=.25, random_state =0)\n",
    "\n",
    "# # Running and storing each algorithm predictions\n",
    "\n",
    "# # SVDpp\n",
    "# algo = SVDpp()\n",
    "# algo.fit(trainset)\n",
    "# svdpp_predictions = algo.test(testset)\n",
    "# # Dump to save predictions\n",
    "# dump.dump('SVDpp_predictions', svdpp_predictions)\n",
    "# SVDpp_predictions = dump.load('probe_set_list/SVDpp_predictions')[0]\n",
    "# # Check accuracy\n",
    "# SVDpp_accuracy = accuracy.rmse(SVDpp_predictions)\n",
    "# # Coverting it into a dataframe\n",
    "# SVDpp_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(SVDpp_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final SVD Predictions Dataframe\n",
    "# SVDpp_predictions_df['movie_id'] = movie_id\n",
    "# SVDpp_predictions_df['user_id'] = user_id\n",
    "# SVDpp_predictions_df['ratings'] = ratings\n",
    "# # Saving predictions_df\n",
    "# pickle_out = open(\"SVDpp_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(SVDpp_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # SVD\n",
    "# algo = SVD()\n",
    "# algo.fit(trainset)\n",
    "# svd_predictions = algo.test(testset)\n",
    "# # Dump to save predictions\n",
    "# dump.dump('SVD_predictions', svd_predictions)\n",
    "# SVD_predictions = dump.load('SVD_predictions')[0]\n",
    "# # Check accuracy\n",
    "# SVD_accuracy = accuracy.rmse(SVD_predictions)\n",
    "# # Coverting it into a dataframe\n",
    "# SVD_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(SVD_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final SVD Predictions Dataframe\n",
    "# SVD_predictions_df['movie_id'] = movie_id\n",
    "# SVD_predictions_df['user_id'] = user_id\n",
    "# SVD_predictions_df['ratings'] = ratings\n",
    "# # Saving predictions_df\n",
    "# pickle_out = open(\"SVD_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(SVD_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "    \n",
    "# # NMF\n",
    "# algo = NMF()\n",
    "# algo.fit(trainset)\n",
    "# nmf_predictions = algo.test(testset)\n",
    "# # Dump to save predictions\n",
    "# dump.dump('NMF_predictions', nmf_predictions)\n",
    "# NMF_predictions = dump.load('NMF_predictions')[0]\n",
    "# # Check accuracy\n",
    "# NMF_accuracy = accuracy.rmse(NMF_predictions)\n",
    "# # Coverting it into a dataframe\n",
    "# NMF_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(NMF_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final NMF Predictions Dataframe\n",
    "# NMF_predictions_df['movie_id'] = movie_id\n",
    "# NMF_predictions_df['user_id'] = user_id\n",
    "# NMF_predictions_df['ratings'] = ratings\n",
    "# # Saving predictions_df\n",
    "# pickle_out = open(\"NMF_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(NMF_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # SlopeOne\n",
    "# algo = SlopeOne()\n",
    "# algo.fit(trainset)\n",
    "# slopeone_predictions = algo.test(testset)\n",
    "# # Dump to save predictions\n",
    "# dump.dump('SlopeOne_predictions', slopeone_predictions)\n",
    "# SlopeOne_predictions = dump.load('SlopeOne_predictions')[0]\n",
    "# # Check accuracy\n",
    "# SlopeOne_accuracy = accuracy.rmse(SlopeOne_predictions)\n",
    "# # Coverting it into a dataframe\n",
    "# SlopeOne_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(SlopeOne_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final SlopeOne Predictions Dataframe\n",
    "# SlopeOne_predictions_df['movie_id'] = movie_id\n",
    "# SlopeOne_predictions_df['user_id'] = user_id\n",
    "# SlopeOne_predictions_df['ratings'] = ratings\n",
    "# # Saving predictions_df\n",
    "# pickle_out = open(\"SlopeOne_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(SlopeOne_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # KNNBasic\n",
    "# algo = KNNBasic()\n",
    "# algo.fit(trainset)\n",
    "# knnbasic_predictions = algo.test(testset)\n",
    "# # Dump to save predictions\n",
    "# dump.dump('KNNBasic_predictions', knnbasic_predictions)\n",
    "# KNNBasic_predictions = dump.load('KNNBasic_predictions')[0]\n",
    "# # Check accuracy\n",
    "# KNNBasic_accuracy = accuracy.rmse(KNNBasic_predictions)\n",
    "# # Coverting it into a dataframe\n",
    "# KNNBasic_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(KNNBasic_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final KNNBasic Predictions Dataframe\n",
    "# KNNBasic_predictions_df['movie_id'] = movie_id\n",
    "# KNNBasic_predictions_df['user_id'] = user_id\n",
    "# KNNBasic_predictions_df['ratings'] = ratings\n",
    "# # Saving predictions_df\n",
    "# pickle_out = open(\"KNNBasic_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(KNNBasic_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # KNNWithMeans\n",
    "# algo = KNNWithMeans()\n",
    "# algo.fit(trainset)\n",
    "# knnwithmeans_predictions = algo.test(testset)\n",
    "# # Dump to save predictions\n",
    "# dump.dump('KNNWithMeans_predictions', knnwithmeans_predictions)\n",
    "# KNNWithMeans_predictions = dump.load('KNNWithMeans_predictions')[0]\n",
    "# # Check accuracy\n",
    "# KNNWithMeans_accuracy = accuracy.rmse(KNNWithMeans_predictions)\n",
    "# # Coverting it into a dataframe\n",
    "# KNNWithMeans_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(KNNWithMeans_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final KNNWithMeans Predictions Dataframe\n",
    "# KNNWithMeans_predictions_df['movie_id'] = movie_id\n",
    "# KNNWithMeans_predictions_df['user_id'] = user_id\n",
    "# KNNWithMeans_predictions_df['ratings'] = ratings\n",
    "# # Saving predictions_df\n",
    "# pickle_out = open(\"KNNWithMeans_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(KNNWithMeans_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # KNNBaseline\n",
    "# algo = KNNBaseline()\n",
    "# algo.fit(trainset)\n",
    "# knnbaseline_predictions = algo.test(testset)\n",
    "# # Dump to save predictions\n",
    "# dump.dump('KNNBaseline_predictions', knnbaseline_predictions)\n",
    "# KNNBaseline_predictions = dump.load('KNNBaseline_predictions')[0]\n",
    "# # Check accuracy\n",
    "# KNNBaseline_accuracy = accuracy.rmse(KNNBaseline_predictions)\n",
    "# # Coverting it into a dataframe\n",
    "# KNNBaseline_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(KNNBaseline_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final KNNBaseline Predictions Dataframe\n",
    "# KNNBaseline_predictions_df['movie_id'] = movie_id\n",
    "# KNNBaseline_predictions_df['user_id'] = user_id\n",
    "# KNNBaseline_predictions_df['ratings'] = ratings\n",
    "# # Saving predictions_df\n",
    "# pickle_out = open(\"KNNBaseline_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(KNNBaseline_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # CoClustering\n",
    "# algo = CoClustering()\n",
    "# algo.fit(trainset)\n",
    "# coclustering_predictions = algo.test(testset)\n",
    "# # Dump to save predictions\n",
    "# dump.dump('CoClustering_predictions', coclustering_predictions)\n",
    "# CoClustering_predictions = dump.load('CoClustering_predictions')[0]\n",
    "# # Check accuracy\n",
    "# CoClustering_accuracy = accuracy.rmse(CoClustering_predictions)\n",
    "# # Coverting it into a dataframe\n",
    "# CoClustering_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(CoClustering_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final CoClustering Predictions Dataframe\n",
    "# CoClustering_predictions_df['movie_id'] = movie_id\n",
    "# CoClustering_predictions_df['user_id'] = user_id\n",
    "# CoClustering_predictions_df['ratings'] = ratings\n",
    "# # Saving predictions_df\n",
    "# pickle_out = open(\"CoClustering_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(CoClustering_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # Support (Log(Number of ratings per user))\n",
    "# num_user_ratings = dict(Counter(netflix_train_df['userID']))\n",
    "# userID = list(num_user_ratings.keys())\n",
    "# support = list(num_user_ratings.values())\n",
    "# l_support = [math.log(i) for i in support]\n",
    "# num_user_ratings_df = pd.DataFrame()\n",
    "# num_user_ratings_df['user_id'] = userID\n",
    "# num_user_ratings_df['support'] = l_support\n",
    "# pickle_out = open(\"num_user_ratings_df.pickle\",\"wb\")\n",
    "# pickle.dump(num_user_ratings_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # Importing all df\n",
    "# pickle_in = open(\"probe_set/SVD_predictions_df.pickle\",\"rb\")\n",
    "# SVD_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"probe_set/NMF_predictions_df.pickle\",\"rb\")\n",
    "# NMF_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"probe_set/SlopeOne_predictions_df.pickle\",\"rb\")\n",
    "# SlopeOne_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"probe_set/KNNBasic_predictions_df.pickle\",\"rb\")\n",
    "# KNNBasic_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"probe_set/KNNWithMeans_predictions_df.pickle\",\"rb\")\n",
    "# KNNWithMeans_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"probe_set/KNNBaseline_predictions_df.pickle\",\"rb\")\n",
    "# KNNBaseline_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"probe_set/CoClustering_predictions_df.pickle\",\"rb\")\n",
    "# CoClustering_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"probe_set/SVDpp_predictions_df.pickle\",\"rb\")\n",
    "# SVDpp_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"additional_info/num_user_ratings_df.pickle\",\"rb\")\n",
    "# num_user_ratings_df = pickle.load(pickle_in)\n",
    "\n",
    "# # Blending the predictions of all the algorithms using linear regression.\n",
    "# all_pred_df = pd.merge(SVD_predictions_df, NMF_predictions_df, on=['movie_id', 'user_id'])\n",
    "# all_pred_df = pd.merge(all_pred_df, SlopeOne_predictions_df, on=['movie_id', 'user_id'])\n",
    "# all_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', 'slope1_ratings']\n",
    "# all_pred_df = pd.merge(all_pred_df, KNNBasic_predictions_df, on=['movie_id', 'user_id'])\n",
    "# all_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', 'slope1_ratings', 'knn_basic_ratings']\n",
    "# all_pred_df = pd.merge(all_pred_df, KNNWithMeans_predictions_df, on=['movie_id', 'user_id'])\n",
    "# all_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', 'slope1_ratings', 'knn_basic_ratings', 'knn_means_ratings']\n",
    "# all_pred_df = pd.merge(all_pred_df, CoClustering_predictions_df, on=['movie_id', 'user_id'])\n",
    "# all_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', \n",
    "#                        'slope1_ratings', 'knn_basic_ratings', 'knn_means_ratings', 'coclustering_ratings']\n",
    "# all_pred_df = pd.merge(all_pred_df, KNNBaseline_predictions_df, on=['movie_id', 'user_id'])\n",
    "# all_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', \n",
    "#                        'slope1_ratings', 'knn_basic_ratings', 'knn_means_ratings', 'knn_baseline_ratings', 'coclustering_ratings']\n",
    "# all_pred_df = pd.merge(all_pred_df, SVDpp_predictions_df, on=['movie_id', 'user_id'])\n",
    "# all_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', \n",
    "#                        'slope1_ratings', 'knn_basic_ratings', 'knn_means_ratings', \n",
    "#                        'knn_baseline_ratings', 'coclustering_ratings', 'svdpp_ratings']\n",
    "# all_pred_df = pd.merge(all_pred_df, num_user_ratings_df, on=['user_id'], how = 'left')\n",
    "\n",
    "# # Extracting Actual ratings\n",
    "# SVD_predictions = dump.load('probe_set_list/SVD_predictions')[0]\n",
    "# actual_ratings = []\n",
    "# for index,i in enumerate(SVD_predictions):\n",
    "#     actual_ratings.append(i[2])\n",
    "\n",
    "# all_pred_df['actual_ratings'] = actual_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Overview of the Probe set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support column is the number of ratings given per user. This variable was inspired by [(Jahrer, Töscher and Legenstein, 2010)](#Jahrer,-M.,-Töscher,-A.-and-Legenstein,-R.-2010.-Combining-Predictions-for-Accurate-Recommender-Systems.-[ebook]-commendo.-Available-at:---)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T13:36:36.764605Z",
     "start_time": "2018-06-07T13:36:36.623289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>svd_ratings</th>\n",
       "      <th>nmf_ratings</th>\n",
       "      <th>slope1_ratings</th>\n",
       "      <th>knn_basic_ratings</th>\n",
       "      <th>knn_means_ratings</th>\n",
       "      <th>knn_baseline_ratings</th>\n",
       "      <th>coclustering_ratings</th>\n",
       "      <th>svdpp_ratings</th>\n",
       "      <th>support</th>\n",
       "      <th>actual_ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11613</td>\n",
       "      <td>2609372</td>\n",
       "      <td>3.166086</td>\n",
       "      <td>4.135768</td>\n",
       "      <td>3.813893</td>\n",
       "      <td>3.926309</td>\n",
       "      <td>4.005082</td>\n",
       "      <td>4.006052</td>\n",
       "      <td>3.986574</td>\n",
       "      <td>3.169665</td>\n",
       "      <td>4.644391</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12246</td>\n",
       "      <td>1331636</td>\n",
       "      <td>3.853815</td>\n",
       "      <td>3.873442</td>\n",
       "      <td>3.830789</td>\n",
       "      <td>3.892531</td>\n",
       "      <td>3.789381</td>\n",
       "      <td>4.051368</td>\n",
       "      <td>3.854374</td>\n",
       "      <td>3.856860</td>\n",
       "      <td>4.574711</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14712</td>\n",
       "      <td>370037</td>\n",
       "      <td>3.320655</td>\n",
       "      <td>3.018145</td>\n",
       "      <td>3.261744</td>\n",
       "      <td>3.022380</td>\n",
       "      <td>2.895497</td>\n",
       "      <td>3.189392</td>\n",
       "      <td>3.016825</td>\n",
       "      <td>3.214281</td>\n",
       "      <td>4.369448</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15998</td>\n",
       "      <td>511899</td>\n",
       "      <td>2.800305</td>\n",
       "      <td>2.973909</td>\n",
       "      <td>2.955995</td>\n",
       "      <td>2.881675</td>\n",
       "      <td>2.952963</td>\n",
       "      <td>2.676285</td>\n",
       "      <td>2.996145</td>\n",
       "      <td>2.878760</td>\n",
       "      <td>5.318120</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15234</td>\n",
       "      <td>1033433</td>\n",
       "      <td>3.100102</td>\n",
       "      <td>3.052640</td>\n",
       "      <td>2.989523</td>\n",
       "      <td>3.202394</td>\n",
       "      <td>3.159956</td>\n",
       "      <td>3.103535</td>\n",
       "      <td>3.121418</td>\n",
       "      <td>3.128072</td>\n",
       "      <td>5.389072</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13853</td>\n",
       "      <td>2205259</td>\n",
       "      <td>2.988051</td>\n",
       "      <td>3.095834</td>\n",
       "      <td>3.156031</td>\n",
       "      <td>3.326073</td>\n",
       "      <td>3.266752</td>\n",
       "      <td>3.049487</td>\n",
       "      <td>3.303883</td>\n",
       "      <td>3.076864</td>\n",
       "      <td>5.327876</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6085</td>\n",
       "      <td>2636960</td>\n",
       "      <td>4.072065</td>\n",
       "      <td>3.041911</td>\n",
       "      <td>3.083444</td>\n",
       "      <td>3.804249</td>\n",
       "      <td>3.422266</td>\n",
       "      <td>3.399480</td>\n",
       "      <td>3.431427</td>\n",
       "      <td>4.238767</td>\n",
       "      <td>4.890349</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11673</td>\n",
       "      <td>1631698</td>\n",
       "      <td>3.118224</td>\n",
       "      <td>3.064367</td>\n",
       "      <td>3.287517</td>\n",
       "      <td>3.505740</td>\n",
       "      <td>3.485114</td>\n",
       "      <td>3.691188</td>\n",
       "      <td>3.445049</td>\n",
       "      <td>3.124536</td>\n",
       "      <td>4.634729</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>442</td>\n",
       "      <td>7949</td>\n",
       "      <td>4.025703</td>\n",
       "      <td>3.563398</td>\n",
       "      <td>3.714719</td>\n",
       "      <td>3.758754</td>\n",
       "      <td>3.463835</td>\n",
       "      <td>3.647633</td>\n",
       "      <td>3.665473</td>\n",
       "      <td>4.117737</td>\n",
       "      <td>4.330733</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2375</td>\n",
       "      <td>2323668</td>\n",
       "      <td>3.265965</td>\n",
       "      <td>3.239913</td>\n",
       "      <td>3.084188</td>\n",
       "      <td>3.231530</td>\n",
       "      <td>3.103762</td>\n",
       "      <td>3.115611</td>\n",
       "      <td>3.187758</td>\n",
       "      <td>3.348662</td>\n",
       "      <td>4.532599</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id  user_id  svd_ratings  nmf_ratings  slope1_ratings  \\\n",
       "0     11613  2609372     3.166086     4.135768        3.813893   \n",
       "1     12246  1331636     3.853815     3.873442        3.830789   \n",
       "2     14712   370037     3.320655     3.018145        3.261744   \n",
       "3     15998   511899     2.800305     2.973909        2.955995   \n",
       "4     15234  1033433     3.100102     3.052640        2.989523   \n",
       "5     13853  2205259     2.988051     3.095834        3.156031   \n",
       "6      6085  2636960     4.072065     3.041911        3.083444   \n",
       "7     11673  1631698     3.118224     3.064367        3.287517   \n",
       "8       442     7949     4.025703     3.563398        3.714719   \n",
       "9      2375  2323668     3.265965     3.239913        3.084188   \n",
       "\n",
       "   knn_basic_ratings  knn_means_ratings  knn_baseline_ratings  \\\n",
       "0           3.926309           4.005082              4.006052   \n",
       "1           3.892531           3.789381              4.051368   \n",
       "2           3.022380           2.895497              3.189392   \n",
       "3           2.881675           2.952963              2.676285   \n",
       "4           3.202394           3.159956              3.103535   \n",
       "5           3.326073           3.266752              3.049487   \n",
       "6           3.804249           3.422266              3.399480   \n",
       "7           3.505740           3.485114              3.691188   \n",
       "8           3.758754           3.463835              3.647633   \n",
       "9           3.231530           3.103762              3.115611   \n",
       "\n",
       "   coclustering_ratings  svdpp_ratings   support  actual_ratings  \n",
       "0              3.986574       3.169665  4.644391             3.0  \n",
       "1              3.854374       3.856860  4.574711             4.0  \n",
       "2              3.016825       3.214281  4.369448             3.0  \n",
       "3              2.996145       2.878760  5.318120             2.0  \n",
       "4              3.121418       3.128072  5.389072             3.0  \n",
       "5              3.303883       3.076864  5.327876             3.0  \n",
       "6              3.431427       4.238767  4.890349             4.0  \n",
       "7              3.445049       3.124536  4.634729             4.0  \n",
       "8              3.665473       4.117737  4.330733             4.0  \n",
       "9              3.187758       3.348662  4.532599             3.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_in = open(\"probe_set/all_pred_df.pickle\",\"rb\")\n",
    "all_pred_df = pickle.load(pickle_in)\n",
    "all_pred_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Blending the predicitons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Creating probe set\n",
    "# probe_set = all_pred_df.copy()\n",
    "\n",
    "# # Division for neural net CV\n",
    "# full_x = probe_set.drop(['actual_ratings', 'movie_id', 'user_id'], axis=1)\n",
    "# full_y = probe_set.iloc[:, [-1]]\n",
    "\n",
    "# # Divding probe set into train and test probe set\n",
    "# probe_set_train = probe_set.iloc[:int(len(probe_set)/2), :]\n",
    "# probe_set_test = probe_set.iloc[int(len(probe_set)/2):, :]\n",
    "\n",
    "# # Train the linear regression blending algorithm\n",
    "# train_x = probe_set_train.drop(['actual_ratings', 'movie_id', 'user_id'], axis=1)\n",
    "# train_y = probe_set_train.iloc[:, [-1]]\n",
    "\n",
    "# test_x = probe_set_test.drop(['actual_ratings', 'movie_id', 'user_id'], axis=1)\n",
    "# test_y = probe_set_test.iloc[:, [-1]]\n",
    "\n",
    "# # Linear Regression\n",
    "# model = sm.OLS(train_y, train_x).fit()\n",
    "# model.summary()\n",
    "# y_hat = model.predict(test_x) # make the predictions by the model\n",
    "# lm_rmse = (mean_squared_error(test_y, y_hat))**0.5\n",
    "# lm_rmse_rounded = (mean_squared_error(test_y, round(y_hat)))**0.5\n",
    "\n",
    "# # cross_val\n",
    "# reg = linear_model.LinearRegression()\n",
    "# seed = 0\n",
    "# kfold = skKFold(n_splits=2, random_state=seed)\n",
    "# lm_results = abs(cross_val_score(reg, full_x, full_y, cv=kfold, scoring = 'mean_squared_error'))**0.5\n",
    "\n",
    "# # Multinomial Logit\n",
    "# mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg').fit(train_x, train_y)\n",
    "# mul_lr_y_hat = mul_lr.predict(test_x)\n",
    "# mul_lr_rmse = metrics.mean_squared_error(test_y, mul_lr_y_hat)**0.5\n",
    "\n",
    "# # Negative Binomail Regression\n",
    "# nb = statsmodels.discrete.discrete_model.NegativeBinomial(train_y, train_x).fit()\n",
    "# nb_y_hat = nb.predict(test_x)\n",
    "# nb_rmse = mean_squared_error(test_y, nb_y_hat)**0.5\n",
    "\n",
    "# # Nueral network\n",
    "# scaler = StandardScaler()  \n",
    "# scaler.fit(train_x) \n",
    "# train_x_s = scaler.transform(train_x)\n",
    "# test_x_s =  scaler.transform(test_x)\n",
    "# clf = MLPRegressor(hidden_layer_sizes=(15,), random_state=1, warm_start=True)\n",
    "# clf.fit(train_x, train_y)\n",
    "# nn_y_hat = clf.predict(test_x)\n",
    "# nn_rmse = mean_squared_error(test_y, nn_y_hat)**0.5\n",
    "\n",
    "# # Comparing rmse of all algorithms\n",
    "# svd_rmse = (mean_squared_error(test_y, test_x.iloc[:,0]))**0.5\n",
    "# nmf_rmse = (mean_squared_error(test_y, test_x.iloc[:,1]))**0.5\n",
    "# slope1_rmse = (mean_squared_error(test_y, test_x.iloc[:,2]))**0.5\n",
    "# knn_basic_rmse = (mean_squared_error(test_y, test_x.iloc[:,3]))**0.5\n",
    "# knn_means_rmse = (mean_squared_error(test_y, test_x.iloc[:,4]))**0.5\n",
    "# knn_baseline_rmse = (mean_squared_error(test_y, test_x.iloc[:,5]))**0.5\n",
    "# co_clustering_rmse = (mean_squared_error(test_y, test_x.iloc[:,6]))**0.5\n",
    "# svdpp_rmse = (mean_squared_error(test_y, test_x.iloc[:,7]))**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation below gives an overiew of the performance of different algorithms as well as the different blenders used to combine the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T13:32:57.234115Z",
     "start_time": "2018-06-07T13:32:57.115888Z"
    }
   },
   "source": [
    "![RMSE Results](viz/result_rmse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Final Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was used to train all the algorithms on the full training set and predictions for the final test set were obtained. These predictions along with the support variable will be used as an input into the trained linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally predicting netflix_test_ratings after training on all the train set\n",
    "\n",
    "\n",
    "# # Data prep\n",
    "# # Train\n",
    "# trainset_full = data.build_full_trainset()\n",
    "\n",
    "# # Test\n",
    "# test_final = netflix_test_df.iloc[:, [0,1]]\n",
    "# # data prep\n",
    "# cols = test_final.columns.tolist()\n",
    "# reordered_cols = ['user_id', 'movie_id']\n",
    "# test_final = test_final[reordered_cols]\n",
    "# test_final.columns = ['userID', 'itemID']\n",
    "\n",
    "\n",
    "# # Algorithms\n",
    "# # CoClustering\n",
    "# # fit\n",
    "# cc_algo = CoClustering()\n",
    "# cc_algo.fit(trainset_full)\n",
    "# # dump algo\n",
    "# # dump.dump('CC_algo', cc_algo)\n",
    "# # load algo\n",
    "# cc_algo = dump.load('final_algo/CC_algo')[0]\n",
    "# # predictions\n",
    "# cc_algo_final_predictions = []\n",
    "# for index, row in test_final.iterrows():\n",
    "#     cc_algo_final_predictions.append(cc_algo.predict(row['userID'], row['itemID']))\n",
    "# # save predictions\n",
    "# dump.dump('cc_algo_final_predictions', cc_algo_final_predictions)\n",
    "# # load predictions\n",
    "# cc_algo_final_predictions = dump.load('final_predictions_list/cc_algo_final_predictions')[0]\n",
    "# # Coverting it into a dataframe\n",
    "# cc_algo_final_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(cc_algo_final_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final CC Predictions Dataframe\n",
    "# cc_algo_final_predictions_df['movie_id'] = movie_id\n",
    "# cc_algo_final_predictions_df['user_id'] = user_id\n",
    "# cc_algo_final_predictions_df['ratings'] = ratings\n",
    "# # Save final df\n",
    "# pickle_out = open(\"cc_algo_final_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(cc_algo_final_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # SVD\n",
    "# svd_algo = SVD()\n",
    "# svd_algo.fit(trainset_full)\n",
    "# # dump algo\n",
    "# # dump.dump('svd_algo', svd_algo)\n",
    "# # load algo\n",
    "# svd_algo = dump.load('final_algo/svd_algo')[0]\n",
    "# # predictions\n",
    "# svd_algo_final_predictions = []\n",
    "# for index, row in test_final.iterrows():\n",
    "#     svd_algo_final_predictions.append(svd_algo.predict(row['userID'], row['itemID']))\n",
    "# # save predictions\n",
    "# dump.dump('svd_algo_final_predictions', svd_algo_final_predictions)\n",
    "# # load predictions\n",
    "# svd_algo_final_predictions = dump.load('final_predictions_list/svd_algo_final_predictions')[0]\n",
    "# # Coverting it into a dataframe\n",
    "# svd_algo_final_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(svd_algo_final_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final SVD Predictions Dataframe\n",
    "# svd_algo_final_predictions_df['movie_id'] = movie_id\n",
    "# svd_algo_final_predictions_df['user_id'] = user_id\n",
    "# svd_algo_final_predictions_df['ratings'] = ratings\n",
    "# # Save final df\n",
    "# pickle_out = open(\"svd_algo_final_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(svd_algo_final_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # SVDpp\n",
    "# svdpp_algo = SVDpp()\n",
    "# svdpp_algo.fit(trainset_full)\n",
    "# # predictions\n",
    "# svdpp_algo_final_predictions = []\n",
    "# for index, row in test_final.iterrows():\n",
    "#     svdpp_algo_final_predictions.append(svdpp_algo.predict(row['userID'], row['itemID']))\n",
    "# # save predictions\n",
    "# dump.dump('svdpp_algo_final_predictions', svdpp_algo_final_predictions)\n",
    "# # load predictions\n",
    "# svdpp_algo_final_predictions = dump.load('svdpp_algo_final_predictions')[0]\n",
    "# # Coverting it into a dataframe\n",
    "# svdpp_algo_final_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(svdpp_algo_final_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final SVD Predictions Dataframe\n",
    "# svdpp_algo_final_predictions_df['movie_id'] = movie_id\n",
    "# svdpp_algo_final_predictions_df['user_id'] = user_id\n",
    "# svdpp_algo_final_predictions_df['ratings'] = ratings\n",
    "# # Save final df\n",
    "# pickle_out = open(\"svdpp_algo_final_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(svdpp_algo_final_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # NMF\n",
    "# nmf_algo = NMF()\n",
    "# nmf_algo.fit(trainset_full)\n",
    "# # dump.dump('nmf_algo', nmf_algo)\n",
    "# # load algo\n",
    "# nmf_algo = dump.load('final_algo/nmf_algo')[0]\n",
    "# # predictions\n",
    "# nmf_algo_final_predictions = []\n",
    "# for index, row in test_final.iterrows():\n",
    "#     nmf_algo_final_predictions.append(nmf_algo.predict(row['userID'], row['itemID']))\n",
    "# # save predictions\n",
    "# dump.dump('nmf_algo_final_predictions', nmf_algo_final_predictions)\n",
    "# # load predictions\n",
    "# nmf_algo_final_predictions = dump.load('final_predictions_list/nmf_algo_final_predictions')[0]\n",
    "# # Coverting it into a dataframe\n",
    "# nmf_algo_final_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(nmf_algo_final_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final SVD Predictions Dataframe\n",
    "# nmf_algo_final_predictions_df['movie_id'] = movie_id\n",
    "# nmf_algo_final_predictions_df['user_id'] = user_id\n",
    "# nmf_algo_final_predictions_df['ratings'] = ratings\n",
    "# # Save final df\n",
    "# pickle_out = open(\"nmf_algo_final_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(nmf_algo_final_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "\n",
    "# # Slope1\n",
    "# sl1_algo = SlopeOne()\n",
    "# sl1_algo.fit(trainset_full)\n",
    "# # dump.dump('sl1_algo', sl1_algo)\n",
    "# # load algo\n",
    "# sl1_algo = dump.load('final_algo/sl1_algo')[0]\n",
    "# # predictions\n",
    "# sl1_algo_final_predictions = []\n",
    "# for index, row in test_final.iterrows():\n",
    "#     sl1_algo_final_predictions.append(sl1_algo.predict(row['userID'], row['itemID']))\n",
    "# # save predictions\n",
    "# dump.dump('sl1_algo_final_predictions', sl1_algo_final_predictions)\n",
    "# # load predictions\n",
    "# sl1_algo_final_predictions = dump.load('final_predictions_list/sl1_algo_final_predictions')[0]\n",
    "# # Coverting it into a dataframe\n",
    "# sl1_algo_final_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(sl1_algo_final_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final SlopeOne Predictions Dataframe\n",
    "# sl1_algo_final_predictions_df['movie_id'] = movie_id\n",
    "# sl1_algo_final_predictions_df['user_id'] = user_id\n",
    "# sl1_algo_final_predictions_df['ratings'] = ratings\n",
    "# # Save final df\n",
    "# pickle_out = open(\"sl1_algo_final_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(sl1_algo_final_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # KNNBasic\n",
    "# # fit\n",
    "# knn_basic_algo = KNNBasic()\n",
    "# knn_basic_algo.fit(trainset_full)\n",
    "# # predictions\n",
    "# knn_basic_algo_final_predictions = []\n",
    "# for index, row in test_final.iterrows():\n",
    "#     knn_basic_algo_final_predictions.append(knn_basic_algo.predict(row['userID'], row['itemID']))\n",
    "# # save predictions\n",
    "# dump.dump('knn_basic_algo_final_predictions', knn_basic_algo_final_predictions)\n",
    "# # load predictions\n",
    "# knn_basic_algo_final_predictions = dump.load('final_predictions_list/knn_basic_algo_final_predictions')[0]\n",
    "# # Coverting it into a dataframe\n",
    "# knn_basic_algo_final_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(knn_basic_algo_final_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final KNNBasic Predictions Dataframe\n",
    "# knn_basic_algo_final_predictions_df['movie_id'] = movie_id\n",
    "# knn_basic_algo_final_predictions_df['user_id'] = user_id\n",
    "# knn_basic_algo_final_predictions_df['ratings'] = ratings\n",
    "# # Save final df\n",
    "# pickle_out = open(\"knn_basic_algo_final_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(knn_basic_algo_final_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # KNNMeans\n",
    "# # fit\n",
    "# knn_means_algo = KNNWithMeans()\n",
    "# knn_means_algo.fit(trainset_full)\n",
    "# # predict\n",
    "# knn_means_algo_final_predictions = []\n",
    "# for index, row in test_final.iterrows():\n",
    "#     knn_means_algo_final_predictions.append(knn_means_algo.predict(row['userID'], row['itemID']))\n",
    "# # save predictions\n",
    "# dump.dump('knn_means_algo_final_predictions', knn_means_algo_final_predictions)\n",
    "# # load predictions\n",
    "# knn_means_algo_final_predictions = dump.load('final_predictions_list/knn_means_algo_final_predictions')[0]\n",
    "# # Coverting it into a dataframe\n",
    "# knn_means_algo_final_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(knn_means_algo_final_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final KNNMeans Predictions Dataframe\n",
    "# knn_means_algo_final_predictions_df['movie_id'] = movie_id\n",
    "# knn_means_algo_final_predictions_df['user_id'] = user_id\n",
    "# knn_means_algo_final_predictions_df['ratings'] = ratings\n",
    "# # Save final df\n",
    "# pickle_out = open(\"knn_means_algo_final_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(knn_means_algo_final_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# # KNNBaseline\n",
    "# # fit\n",
    "# knn_baseline_algo = KNNBaseline()\n",
    "# knn_baseline_algo.fit(trainset_full)\n",
    "# # predict\n",
    "# knn_baseline_algo_final_predictions = []\n",
    "# for index, row in test_final.iterrows():\n",
    "#     knn_baseline_algo_final_predictions.append(knn_baseline_algo.predict(row['userID'], row['itemID']))\n",
    "# # save predictions\n",
    "# dump.dump('knn_baseline_algo_final_predictions', knn_baseline_algo_final_predictions)\n",
    "# # load predictions\n",
    "# knn_baseline_algo_final_predictions = dump.load('final_predictions_list/knn_baseline_algo_final_predictions')[0]\n",
    "# # Coverting it into a dataframe\n",
    "# knn_baseline_algo_final_predictions_df = pd.DataFrame()\n",
    "# user_id = list()\n",
    "# movie_id = list()\n",
    "# ratings = list()\n",
    "# # Append\n",
    "# for index,i in enumerate(knn_baseline_algo_final_predictions):\n",
    "#     user_id.append(i[0])\n",
    "#     movie_id.append(i[1])\n",
    "#     ratings.append(i[3])\n",
    "# # Final KNNBaseline Predictions Dataframe\n",
    "# knn_baseline_algo_final_predictions_df['movie_id'] = movie_id\n",
    "# knn_baseline_algo_final_predictions_df['user_id'] = user_id\n",
    "# knn_baseline_algo_final_predictions_df['ratings'] = ratings\n",
    "# # Save final df\n",
    "# pickle_out = open(\"knn_baseline_algo_final_predictions_df.pickle\",\"wb\")\n",
    "# pickle.dump(knn_baseline_algo_final_predictions_df, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# Final predictions\n",
    "\n",
    "# # Loading all the df\n",
    "# pickle_in = open(\"final_predictions/svd_algo_final_predictions_df.pickle\",\"rb\")\n",
    "# svd_algo_final_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"final_predictions/nmf_algo_final_predictions_df.pickle\",\"rb\")\n",
    "# nmf_algo_final_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"final_predictions/sl1_algo_final_predictions_df.pickle\",\"rb\")\n",
    "# sl1_algo_final_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"final_predictions/knn_basic_algo_final_predictions_df.pickle\",\"rb\")\n",
    "# knn_basic_algo_final_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"final_predictions/knn_means_algo_final_predictions_df.pickle\",\"rb\")\n",
    "# knn_means_algo_final_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"final_predictions/knn_baseline_algo_final_predictions_df.pickle\",\"rb\")\n",
    "# knn_baseline_algo_final_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"final_predictions/cc_algo_final_predictions_df.pickle\",\"rb\")\n",
    "# cc_algo_final_predictions_df = pickle.load(pickle_in)\n",
    "# pickle_in = open(\"final_predictions/svdpp_algo_final_predictions_df.pickle\",\"rb\")\n",
    "# svdpp_algo_final_predictions_df = pickle.load(pickle_in)\n",
    "\n",
    "# # Combining all the predictions\n",
    "# final_pred_df = pd.merge(svd_algo_final_predictions_df, nmf_algo_final_predictions_df, on=['movie_id', 'user_id'])\n",
    "# final_pred_df = pd.merge(final_pred_df, sl1_algo_final_predictions_df, on=['movie_id', 'user_id'])\n",
    "# final_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', 'slope1_ratings']\n",
    "# final_pred_df = pd.merge(final_pred_df, knn_basic_algo_final_predictions_df, on=['movie_id', 'user_id'])\n",
    "# final_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', 'slope1_ratings', 'knn_basic_ratings']\n",
    "# final_pred_df = pd.merge(final_pred_df, knn_means_algo_final_predictions_df, on=['movie_id', 'user_id'])\n",
    "# final_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', 'slope1_ratings', 'knn_basic_ratings', 'knn_means_ratings']\n",
    "# final_pred_df = pd.merge(final_pred_df, cc_algo_final_predictions_df, on=['movie_id', 'user_id'])\n",
    "# final_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', \n",
    "#                        'slope1_ratings', 'knn_basic_ratings', 'knn_means_ratings', 'coclustering_ratings']\n",
    "# final_pred_df = pd.merge(final_pred_df, knn_baseline_algo_final_predictions_df, on=['movie_id', 'user_id'])\n",
    "# final_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', \n",
    "#                        'slope1_ratings', 'knn_basic_ratings', 'knn_means_ratings', 'knn_baseline_ratings', 'coclustering_ratings']\n",
    "# final_pred_df = pd.merge(final_pred_df, svdpp_algo_final_predictions_df, on=['movie_id', 'user_id'])\n",
    "# final_pred_df.columns = ['movie_id', 'user_id', 'svd_ratings', 'nmf_ratings', \n",
    "#                        'slope1_ratings', 'knn_basic_ratings', 'knn_means_ratings', \n",
    "#                        'knn_baseline_ratings', 'coclustering_ratings', 'svdpp_ratings']\n",
    "# final_pred_df = pd.merge(final_pred_df, num_user_ratings_df, on=['user_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Overiview of the final input dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T13:40:12.412091Z",
     "start_time": "2018-06-07T13:40:12.396690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>svd_ratings</th>\n",
       "      <th>nmf_ratings</th>\n",
       "      <th>slope1_ratings</th>\n",
       "      <th>knn_basic_ratings</th>\n",
       "      <th>knn_means_ratings</th>\n",
       "      <th>knn_baseline_ratings</th>\n",
       "      <th>coclustering_ratings</th>\n",
       "      <th>svdpp_ratings</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>443</td>\n",
       "      <td>1549632</td>\n",
       "      <td>3.900394</td>\n",
       "      <td>3.637745</td>\n",
       "      <td>3.624227</td>\n",
       "      <td>3.950711</td>\n",
       "      <td>3.846193</td>\n",
       "      <td>3.904844</td>\n",
       "      <td>3.861094</td>\n",
       "      <td>4.313123</td>\n",
       "      <td>4.343805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2104</td>\n",
       "      <td>428488</td>\n",
       "      <td>4.366531</td>\n",
       "      <td>3.865321</td>\n",
       "      <td>3.953622</td>\n",
       "      <td>4.106516</td>\n",
       "      <td>4.057675</td>\n",
       "      <td>4.148139</td>\n",
       "      <td>4.122946</td>\n",
       "      <td>4.482713</td>\n",
       "      <td>4.317488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9410</td>\n",
       "      <td>2069695</td>\n",
       "      <td>2.922406</td>\n",
       "      <td>3.121146</td>\n",
       "      <td>3.090248</td>\n",
       "      <td>2.933241</td>\n",
       "      <td>2.993097</td>\n",
       "      <td>3.032869</td>\n",
       "      <td>2.997629</td>\n",
       "      <td>3.288643</td>\n",
       "      <td>4.430817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4005</td>\n",
       "      <td>128311</td>\n",
       "      <td>2.209018</td>\n",
       "      <td>2.701658</td>\n",
       "      <td>2.792136</td>\n",
       "      <td>3.251097</td>\n",
       "      <td>2.739357</td>\n",
       "      <td>3.060909</td>\n",
       "      <td>2.888179</td>\n",
       "      <td>2.214324</td>\n",
       "      <td>5.484797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16770</td>\n",
       "      <td>2037731</td>\n",
       "      <td>4.222026</td>\n",
       "      <td>3.800297</td>\n",
       "      <td>3.994312</td>\n",
       "      <td>3.578243</td>\n",
       "      <td>3.806397</td>\n",
       "      <td>3.796368</td>\n",
       "      <td>3.878822</td>\n",
       "      <td>4.263567</td>\n",
       "      <td>4.418841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4847</td>\n",
       "      <td>316006</td>\n",
       "      <td>2.612429</td>\n",
       "      <td>2.498468</td>\n",
       "      <td>2.686728</td>\n",
       "      <td>3.503344</td>\n",
       "      <td>2.913701</td>\n",
       "      <td>3.046596</td>\n",
       "      <td>2.923438</td>\n",
       "      <td>2.519208</td>\n",
       "      <td>4.276666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8596</td>\n",
       "      <td>2601377</td>\n",
       "      <td>4.236736</td>\n",
       "      <td>4.095032</td>\n",
       "      <td>4.059623</td>\n",
       "      <td>3.977705</td>\n",
       "      <td>3.924006</td>\n",
       "      <td>4.113297</td>\n",
       "      <td>3.897929</td>\n",
       "      <td>4.294060</td>\n",
       "      <td>5.384495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10747</td>\n",
       "      <td>2558018</td>\n",
       "      <td>3.362627</td>\n",
       "      <td>3.593534</td>\n",
       "      <td>3.539345</td>\n",
       "      <td>3.606601</td>\n",
       "      <td>3.624971</td>\n",
       "      <td>3.652347</td>\n",
       "      <td>3.614666</td>\n",
       "      <td>3.435273</td>\n",
       "      <td>4.430817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3418</td>\n",
       "      <td>849966</td>\n",
       "      <td>4.194747</td>\n",
       "      <td>4.014765</td>\n",
       "      <td>4.072536</td>\n",
       "      <td>4.419867</td>\n",
       "      <td>4.388381</td>\n",
       "      <td>4.434302</td>\n",
       "      <td>4.413697</td>\n",
       "      <td>3.885886</td>\n",
       "      <td>4.543295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3638</td>\n",
       "      <td>1019317</td>\n",
       "      <td>2.799889</td>\n",
       "      <td>3.054120</td>\n",
       "      <td>3.026219</td>\n",
       "      <td>3.273829</td>\n",
       "      <td>3.370123</td>\n",
       "      <td>3.262593</td>\n",
       "      <td>3.236138</td>\n",
       "      <td>2.591672</td>\n",
       "      <td>4.382027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id  user_id  svd_ratings  nmf_ratings  slope1_ratings  \\\n",
       "0       443  1549632     3.900394     3.637745        3.624227   \n",
       "1      2104   428488     4.366531     3.865321        3.953622   \n",
       "2      9410  2069695     2.922406     3.121146        3.090248   \n",
       "3      4005   128311     2.209018     2.701658        2.792136   \n",
       "4     16770  2037731     4.222026     3.800297        3.994312   \n",
       "5      4847   316006     2.612429     2.498468        2.686728   \n",
       "6      8596  2601377     4.236736     4.095032        4.059623   \n",
       "7     10747  2558018     3.362627     3.593534        3.539345   \n",
       "8      3418   849966     4.194747     4.014765        4.072536   \n",
       "9      3638  1019317     2.799889     3.054120        3.026219   \n",
       "\n",
       "   knn_basic_ratings  knn_means_ratings  knn_baseline_ratings  \\\n",
       "0           3.950711           3.846193              3.904844   \n",
       "1           4.106516           4.057675              4.148139   \n",
       "2           2.933241           2.993097              3.032869   \n",
       "3           3.251097           2.739357              3.060909   \n",
       "4           3.578243           3.806397              3.796368   \n",
       "5           3.503344           2.913701              3.046596   \n",
       "6           3.977705           3.924006              4.113297   \n",
       "7           3.606601           3.624971              3.652347   \n",
       "8           4.419867           4.388381              4.434302   \n",
       "9           3.273829           3.370123              3.262593   \n",
       "\n",
       "   coclustering_ratings  svdpp_ratings   support  \n",
       "0              3.861094       4.313123  4.343805  \n",
       "1              4.122946       4.482713  4.317488  \n",
       "2              2.997629       3.288643  4.430817  \n",
       "3              2.888179       2.214324  5.484797  \n",
       "4              3.878822       4.263567  4.418841  \n",
       "5              2.923438       2.519208  4.276666  \n",
       "6              3.897929       4.294060  5.384495  \n",
       "7              3.614666       3.435273  4.430817  \n",
       "8              4.413697       3.885886  4.543295  \n",
       "9              3.236138       2.591672  4.382027  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_in = open(\"final_predictions/final_pred_df.pickle\",\"rb\")\n",
    "final_pred_df = pickle.load(pickle_in)\n",
    "final_pred_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T13:41:06.772929Z",
     "start_time": "2018-06-07T13:41:06.769271Z"
    }
   },
   "source": [
    "> **Final Prediction using Linear Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final_test_x = final_pred_df.drop(['user_id', 'movie_id'], axis =1)\n",
    "\n",
    "# # Final Prediction\n",
    "# final_model = sm.OLS(full_y, full_x).fit()\n",
    "# final_model.summary()\n",
    "# final_y_hat = final_model.predict(final_test_x)\n",
    "# final_y_hat = final_y_hat.clip(upper=5)\n",
    "\n",
    "# final_predictions_test =  netflix_test_df.copy()\n",
    "# final_predictions_test.ratings = final_y_hat\n",
    "\n",
    "# np.savetxt('final_test_predictions.text', final_predictions_test, delimiter=',', fmt=['%d','%d', '%10.6f']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final prediction can be found in the final_predictions folder:\n",
    "\n",
    "[final_test_predictions.txt](final_predictions/final_test_predictions.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-04T17:39:09.459170Z",
     "start_time": "2018-06-04T17:39:09.456953Z"
    }
   },
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the computationally expensive code, the parameters for specific recommendation algorithms were not tuned. The deafult set of parameters were used to make the predictions. In order to improve the prediction, one could have tunned the parameters for each algorithm using grid search and then apply the same methodology as explained above. This could possible improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References <a class='tocSkip'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-04T17:21:50.459341Z",
     "start_time": "2018-06-04T17:21:50.448183Z"
    }
   },
   "source": [
    "##### **Jahrer, M., Töscher, A. and Legenstein, R. 2010.** Combining Predictions for Accurate Recommender Systems. [ebook] commendo. Available at:  <a class='tocSkip'> </a> \n",
    "https://pdfs.semanticscholar.org/presentation/8643/a88a780c701c25061c18f6e1301c479b2ea3.pdf \n",
    "\n",
    "[Accessed 4 Jun. 2018].\n",
    "\n",
    "##### **Yeung, A. 2017. Matrix Factorization:** <a class='tocSkip'> </a> A Simple Tutorial and Implementation in Python | Albert Au Yeung. [online] Albertauyeung.com. Available at: \n",
    "http://www.albertauyeung.com/post/python-matrix-factorization/ \n",
    "\n",
    "[Accessed 4 Jun. 2018].\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-24T15:34:33.635404Z",
     "start_time": "2018-05-24T15:34:33.631402Z"
    }
   },
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
